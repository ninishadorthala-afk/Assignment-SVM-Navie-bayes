{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Information Gain, and how is it used in Decision Trees?\n",
        "Answer:\n",
        "- Information Gain measures the reduction in entropy (uncertainty) after splitting a dataset on a feature.\n",
        "- Formula:\n",
        "IG(S,A)=Entropy(S)-\\sum _{v\\in Values(A)}\\frac{|S_v|}{|S|}\\cdot Entropy(S_v)- In Decision Trees, the feature with the highest Information Gain is chosen for splitting, ensuring maximum reduction in impurity.\n"
      ],
      "metadata": {
        "id": "Y8dcAKobn_Fl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the difference between Gini Impurity and Entropy?\n",
        "Answer:\n",
        "- Entropy: Measures disorder using logarithms. Range: 0 (pure) to 1 (highly impure).\n",
        "- Gini Impurity: Measures probability of misclassification. Range: 0 (pure) to 0.5 (max impurity).\n",
        "- Comparison:\n",
        "- Entropy is more computationally expensive (logarithms).\n",
        "- Gini is faster and often preferred in practice.\n",
        "- Both yield similar results; choice depends on preference for speed vs. theoretical rigor.\n"
      ],
      "metadata": {
        "id": "x3q0jZqGoUml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is Pre-Pruning in Decision Trees?\n",
        "Answer:\n",
        "- Pre-pruning stops tree growth early to avoid overfitting.\n",
        "- Techniques:\n",
        "- Limit maximum depth.\n",
        "- Minimum samples per split/leaf.\n",
        "- Stop if Information Gain is below a threshold.\n",
        "- Helps improve generalization and reduces complexity.\n"
      ],
      "metadata": {
        "id": "Z9xsJZXnocSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4. Python Program: Decision Tree Classifier using Gini Impurity\n",
        "#Answer (Code):\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train Decision Tree with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q36VEii8otZ5",
        "outputId": "72a10648-68e9-4854-d826-22d8388696a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.01333333 0.         0.56405596 0.42261071]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is a Support Vector Machine (SVM)?\n",
        "Answer:\n",
        "- SVM is a supervised learning algorithm that finds the optimal hyperplane separating classes with maximum margin.\n",
        "- Works well in high-dimensional spaces and is robust against overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "xQwgAHF1pABT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. What is the Kernel Trick in SVM?\n",
        "Answer:\n",
        "- Kernel Trick allows SVM to classify non-linear data by mapping it into higher dimensions.\n",
        "- Common kernels: Linear, Polynomial, Radial Basis Function (RBF).\n",
        "- Advantage: Avoids explicit computation of high-dimensional transformations.\n"
      ],
      "metadata": {
        "id": "lnBuvHj0pSpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7. Python Program: SVM with Linear and RBF Kernels (Wine Dataset)\n",
        "#Answer (Code):\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    wine.data, wine.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "acc_linear = accuracy_score(y_test, svm_linear.predict(X_test))\n",
        "\n",
        "# RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "acc_rbf = accuracy_score(y_test, svm_rbf.predict(X_test))\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", acc_linear)\n",
        "print(\"RBF Kernel Accuracy:\", acc_rbf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyELgccLpXak",
        "outputId": "ab2332c6-db22-4f44-a98c-cf3be98e2c40"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9814814814814815\n",
            "RBF Kernel Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "Answer:\n",
        "- Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem.\n",
        "- It assumes independence among features, which is rarely true in practice → hence “Naïve.”\n",
        "- Despite the assumption, it performs well in text classification and spam detection.\n",
        "\n"
      ],
      "metadata": {
        "id": "uR_OOOn_pk8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Differences between Gaussian, Multinomial, and Bernoulli Naïve Bayes\n",
        "Answer:\n",
        "- Gaussian NB: Assumes features follow a normal distribution. Best for continuous data.\n",
        "- Multinomial NB: Assumes features are counts/frequencies. Best for text classification (word counts).\n",
        "- Bernoulli NB: Assumes binary features (0/1). Best for presence/absence data (e.g., spam filters).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LHs6K37MpraG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q10. Python Program: Gaussian Naïve Bayes on Breast Cancer Dataset\n",
        "#Answer (Code):\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naive Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate accuracy\n",
        "y_pred = gnb.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28l83OaRpeDP",
        "outputId": "bd3650a3-7d00-4f29-fcd3-7ca633740f77"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9415204678362573\n"
          ]
        }
      ]
    }
  ]
}